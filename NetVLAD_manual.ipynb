{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re-V0EBYOt7s"
      },
      "source": [
        "[***출처: 심현주님의 블로그를 참고했습니다.***](https://medium.com/hyunjulie/%EC%BA%90%EA%B8%80%EA%B3%BC-%EA%B5%AC%EA%B8%80-colab-%EC%97%B0%EA%B2%B0%ED%95%B4%EC%A3%BC%EA%B8%B0-6a274f6de81d)\n",
        "\n",
        "\n",
        "**1. 먼저 본인의 캐글 계정에서 API Token 을 다운받습니다.**\n",
        "\n",
        "![업로드방법](https://cdn-images-1.medium.com/max/800/1*yvocb2Sbyf_ZbBxUcJM7rA.png)\n",
        "\n",
        "**오른쪽 상단에서 My Account 에 들어갑니다**\n",
        "\n",
        "![API받기](https://cdn-images-1.medium.com/max/800/1*JgVnYqQh6mEEoKCLayFz9g.png)\n",
        "\n",
        "**Create New API Token 을 누르면 kaggle.json 파일이 다운로드 됩니다. 이 파일은 찾기 쉬운곳에 일단 옮겨두고,**\n",
        "\n",
        "**아래의 셀을 실행합니다. 여기서 '파일선택'을 누르고 kaggle.json을 업로드합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "vh8xECtdNpm2",
        "outputId": "e87d05b7-5a62-435c-f568-99bb0bae8180"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      2\u001b[0m files\u001b[39m.\u001b[39mupload()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9S3u36HNrXN"
      },
      "outputs": [],
      "source": [
        "# json 파일 옮겨주기\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Permission Warning 이 일어나지 않도록 \n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6MLF36rNcqG",
        "outputId": "c6c536bf-3633-475f-e10d-26daea71f305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading netvlad-landmark-dataset-sejong.zip to /content\n",
            " 47% 740M/1.53G [00:32<00:35, 24.7MB/s]"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d socome/netvlad-landmark-dataset-sejong\n",
        "!unzip netvlad-landmark-dataset-sejong.zip\n",
        "!tar -xvzf Sejong.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbw-FVyCRHpJ"
      },
      "source": [
        "\n",
        "# **다음과 같이 정상적으로 설치 됐는지 확인합니다.**\n",
        "\n",
        "![확인](https://user-images.githubusercontent.com/44772344/58857792-015ac400-86e1-11e9-9cd3-a4f772cfccfe.PNG)\n",
        "\n",
        "\n",
        "**다음으로는 NetVLAD와 TripletNEt을 섞은 EmbedNet을 통해서 학습을 진행할 예정입니다.**\n",
        "\n",
        "![모델](https://user-images.githubusercontent.com/44772344/58701146-24cdf800-83dd-11e9-924d-4e5e247bfec3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxSWZJWjP-jB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "\n",
        "import PIL\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets, metrics\n",
        "\n",
        "torch.manual_seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcrISVFrRn3u"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "tb = TensorBoardColab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrDzaVOFSZpU"
      },
      "source": [
        "## **NetVLAD와 TripletLoss를 결합하는 코드는 다음 깃허브에서 참고했습니다.**\n",
        "\n",
        "# [바로가기](https://github.com/lyakaap/NetVLAD-pytorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wP1v725Ro5P"
      },
      "outputs": [],
      "source": [
        "class NetVLAD(nn.Module):\n",
        "    \"\"\"NetVLAD layer implementation\"\"\"\n",
        "\n",
        "    def __init__(self, num_clusters=6, dim=128, alpha=100.0,\n",
        "                 normalize_input=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_clusters : int\n",
        "                The number of clusters\n",
        "            dim : int\n",
        "                Dimension of descriptors\n",
        "            alpha : float\n",
        "                Parameter of initialization. Larger value is harder assignment.\n",
        "            normalize_input : bool\n",
        "                If true, descriptor-wise L2 normalization is applied to input.\n",
        "        \"\"\"\n",
        "        super(NetVLAD, self).__init__()\n",
        "        self.num_clusters = num_clusters\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.normalize_input = normalize_input\n",
        "        self.conv = nn.Conv2d(dim, num_clusters, kernel_size=(1, 1), bias=True)\n",
        "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
        "        self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.conv.weight = nn.Parameter(\n",
        "            (2.0 * self.alpha * self.centroids).unsqueeze(-1).unsqueeze(-1)\n",
        "        )\n",
        "        self.conv.bias = nn.Parameter(\n",
        "            - self.alpha * self.centroids.norm(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C = x.shape[:2]\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = F.normalize(x, p=2, dim=1)  # across descriptor dim\n",
        "\n",
        "        # soft-assignment\n",
        "        soft_assign = self.conv(x).view(N, self.num_clusters, -1)\n",
        "        soft_assign = F.softmax(soft_assign, dim=1)\n",
        "\n",
        "        x_flatten = x.view(N, C, -1)\n",
        "        \n",
        "        # calculate residuals to each clusters\n",
        "        residual = x_flatten.expand(self.num_clusters, -1, -1, -1).permute(1, 0, 2, 3) - \\\n",
        "            self.centroids.expand(x_flatten.size(-1), -1, -1).permute(1, 2, 0).unsqueeze(0)\n",
        "        residual *= soft_assign.unsqueeze(2)\n",
        "        vlad = residual.sum(dim=-1)\n",
        "\n",
        "        vlad = F.normalize(vlad, p=2, dim=2)  # intra-normalization\n",
        "        vlad = vlad.view(x.size(0), -1)  # flatten\n",
        "        vlad = F.normalize(vlad, p=2, dim=1)  # L2 normalize\n",
        "\n",
        "        return vlad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0-nmvniRqmh"
      },
      "outputs": [],
      "source": [
        "class EmbedNet(nn.Module):\n",
        "    def __init__(self, base_model, net_vlad):\n",
        "        super(EmbedNet, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.net_vlad = net_vlad\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        embedded_x = self.net_vlad(x)\n",
        "        return embedded_x\n",
        "      \n",
        "class TripletNet(nn.Module):\n",
        "    def __init__(self, embed_net):\n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embed_net = embed_net\n",
        "\n",
        "    def forward(self, a, p, n):\n",
        "        embedded_a = self.embed_net(a)\n",
        "        embedded_p = self.embed_net(p)\n",
        "        embedded_n = self.embed_net(n)\n",
        "        return embedded_a, embedded_p, embedded_n\n",
        "\n",
        "    def feature_extract(self, x):\n",
        "        return self.embed_net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXdrzb1YRr_o"
      },
      "outputs": [],
      "source": [
        "class HardTripletLoss(nn.Module):\n",
        "    \"\"\"Hard/Hardest Triplet Loss\n",
        "    (pytorch implementation of https://omoindrot.github.io/triplet-loss)\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=0.1, hardest=False, squared=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            margin: margin for triplet loss\n",
        "            hardest: If true, loss is considered only hardest triplets.\n",
        "            squared: If true, output is the pairwise squared euclidean distance matrix.\n",
        "                If false, output is the pairwise euclidean distance matrix.\n",
        "        \"\"\"\n",
        "        super(HardTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.hardest = hardest\n",
        "        self.squared = squared\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels: labels of the batch, of size (batch_size,)\n",
        "            embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        Returns:\n",
        "            triplet_loss: scalar tensor containing the triplet loss\n",
        "        \"\"\"\n",
        "        pairwise_dist = _pairwise_distance(embeddings, squared=self.squared)\n",
        "\n",
        "        if self.hardest:\n",
        "            # Get the hardest positive pairs\n",
        "            mask_anchor_positive = _get_anchor_positive_triplet_mask(labels).float()\n",
        "            valid_positive_dist = pairwise_dist * mask_anchor_positive\n",
        "            hardest_positive_dist, _ = torch.max(valid_positive_dist, dim=1, keepdim=True)\n",
        "\n",
        "            # Get the hardest negative pairs\n",
        "            mask_anchor_negative = _get_anchor_negative_triplet_mask(labels).float()\n",
        "            max_anchor_negative_dist, _ = torch.max(pairwise_dist, dim=1, keepdim=True)\n",
        "            anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (\n",
        "                    1.0 - mask_anchor_negative)\n",
        "            hardest_negative_dist, _ = torch.min(anchor_negative_dist, dim=1, keepdim=True)\n",
        "\n",
        "            # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "            triplet_loss = F.relu(hardest_positive_dist - hardest_negative_dist + 0.1)\n",
        "            triplet_loss = torch.mean(triplet_loss)\n",
        "        else:\n",
        "            anc_pos_dist = pairwise_dist.unsqueeze(dim=2)\n",
        "            anc_neg_dist = pairwise_dist.unsqueeze(dim=1)\n",
        "\n",
        "            # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
        "            # triplet_loss[i, j, k] will contain the triplet loss of anc=i, pos=j, neg=k\n",
        "            # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
        "            # and the 2nd (batch_size, 1, batch_size)\n",
        "            loss = anc_pos_dist - anc_neg_dist + self.margin\n",
        "\n",
        "            mask = _get_triplet_mask(labels).float()\n",
        "            triplet_loss = loss * mask\n",
        "\n",
        "            # Remove negative losses (i.e. the easy triplets)\n",
        "            triplet_loss = F.relu(triplet_loss)\n",
        "\n",
        "            # Count number of hard triplets (where triplet_loss > 0)\n",
        "            hard_triplets = torch.gt(triplet_loss, 1e-16).float()\n",
        "            num_hard_triplets = torch.sum(hard_triplets)\n",
        "\n",
        "            triplet_loss = torch.sum(triplet_loss) / (num_hard_triplets + 1e-16)\n",
        "\n",
        "        return triplet_loss\n",
        "\n",
        "\n",
        "def _pairwise_distance(x, squared=False, eps=1e-16):\n",
        "    # Compute the 2D matrix of distances between all the embeddings.\n",
        "\n",
        "    cor_mat = torch.matmul(x, x.t())\n",
        "    norm_mat = cor_mat.diag()\n",
        "    distances = norm_mat.unsqueeze(1) - 2 * cor_mat + norm_mat.unsqueeze(0)\n",
        "    distances = F.relu(distances)\n",
        "\n",
        "    if not squared:\n",
        "        mask = torch.eq(distances, 0.0).float()\n",
        "        distances = distances + mask * eps\n",
        "        distances = torch.sqrt(distances)\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    # Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    indices_not_equal = torch.eye(labels.shape[0]).to(device).byte() ^ 1\n",
        "\n",
        "    # Check if labels[i] == labels[j]\n",
        "    labels_equal = torch.unsqueeze(labels, 0) == torch.unsqueeze(labels, 1)\n",
        "\n",
        "    mask = indices_not_equal * labels_equal\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    # Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
        "\n",
        "    # Check if labels[i] != labels[k]\n",
        "    labels_equal = torch.unsqueeze(labels, 0) == torch.unsqueeze(labels, 1)\n",
        "    mask = labels_equal ^ 1\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_triplet_mask(labels):\n",
        "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
        "    A triplet (i, j, k) is valid if:\n",
        "        - i, j, k are distinct\n",
        "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Check that i, j and k are distinct\n",
        "    indices_not_same = torch.eye(labels.shape[0]).to(device).byte() ^ 1\n",
        "    i_not_equal_j = torch.unsqueeze(indices_not_same, 2)\n",
        "    i_not_equal_k = torch.unsqueeze(indices_not_same, 1)\n",
        "    j_not_equal_k = torch.unsqueeze(indices_not_same, 0)\n",
        "    distinct_indices = i_not_equal_j * i_not_equal_k * j_not_equal_k\n",
        "\n",
        "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    label_equal = torch.eq(torch.unsqueeze(labels, 0), torch.unsqueeze(labels, 1))\n",
        "    i_equal_j = torch.unsqueeze(label_equal, 2)\n",
        "    i_equal_k = torch.unsqueeze(label_equal, 1)\n",
        "    valid_labels = i_equal_j * (~i_equal_k)\n",
        "\n",
        "    mask = distinct_indices * valid_labels   # Combine the two masks\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP-XU6tsRtub"
      },
      "outputs": [],
      "source": [
        "# Discard layers at the end of base network\n",
        "encoder = resnet18(pretrained=True)\n",
        "base_model = nn.Sequential(\n",
        "    encoder.conv1,\n",
        "    encoder.bn1,\n",
        "    encoder.relu,\n",
        "    encoder.maxpool,\n",
        "    encoder.layer1,\n",
        "    encoder.layer2,\n",
        "    encoder.layer3,\n",
        "    encoder.layer4,\n",
        ")\n",
        "dim = list(base_model.parameters())[-1].shape[0]  # last channels (512)\n",
        "\n",
        "# Define model for embedding\n",
        "net_vlad = NetVLAD(num_clusters=6, dim=dim, alpha=1.0)\n",
        "model = EmbedNet(base_model, net_vlad).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxG3KaF5TA57"
      },
      "source": [
        "## 실제 학습을 진행하는 부분입니다. \n",
        "#### Epoch = ??/ batch = 8을 설정하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agkolTeaRyKd"
      },
      "outputs": [],
      "source": [
        "# Define loss\n",
        "criterion = HardTripletLoss(margin=0.1).cuda()\n",
        "epochs = 50\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556hHcgDTaFf"
      },
      "source": [
        " #### 'colab oserror errno 5 input/output error' 를 해결하기 위해 \n",
        " #### 모든 데이터를 로드해 캐시에 저장하고 이후에 학습을 진행하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5n_-UPNULpV"
      },
      "outputs": [],
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((128,128)),               \n",
        "    torchvision.transforms.ToTensor()\n",
        "    \n",
        "])\n",
        "\n",
        "bef_train_imagenet_data = torchvision.datasets.ImageFolder('./Sejong/train', transform=transforms)\n",
        "bef_train_data_loader = torch.utils.data.DataLoader(bef_train_imagenet_data,\n",
        "                                          batch_size=776,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdpSb7fmUWwd"
      },
      "outputs": [],
      "source": [
        "for bef_train_image,bef_train_label in bef_train_data_loader :\n",
        "  bef_train_image = bef_train_image\n",
        "  bef_train_label = bef_train_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bB7ahj0SLaF"
      },
      "outputs": [],
      "source": [
        "train_imagenet_data = torchvision.datasets.ImageFolder('./Sejong/train', transform=transforms)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_imagenet_data,\n",
        "                                          batch_size=8,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=0)\n",
        "test_imagenet_data = torchvision.datasets.ImageFolder('./Sejong/test', transform=transforms)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_imagenet_data,\n",
        "                                          shuffle=False,\n",
        "                                          batch_size=100,\n",
        "                                          num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcMSP_G6TW6I"
      },
      "outputs": [],
      "source": [
        "globaliter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, (train_image,train_label) in enumerate(train_data_loader) :\n",
        "    output_train = model(train_image.cuda())\n",
        "    triplet_loss = criterion(output_train, train_label.cuda())\n",
        "    optimizer.zero_grad()\n",
        "    triplet_loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    # This is where I'm recording to Tensorboard\n",
        "    tb.save_value('Train Loss', 'train_loss', globaliter, triplet_loss.item())\n",
        "    print('epoch : {}, globaliter : {}, batch_idx  : {}, triplet_loss : {}'.format(epoch,globaliter,batch_idx,triplet_loss.item()))\n",
        "    globaliter += 1\n",
        "  model_save_name = 'model_{:02d}.pt'.format(epoch)\n",
        "  path = F\"./{model_save_name}\" \n",
        "  torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzoBYf5YaDre"
      },
      "source": [
        "![111](https://user-images.githubusercontent.com/44772344/59032140-850edf00-88a0-11e9-90f6-925901406168.PNG)\n",
        "\n",
        "## 이를 텐서보드로 확인하면 다음과 같습니다.\n",
        "\n",
        "\n",
        "## 이미 학습한 모델을 받는다면 싶다면 다음 명령어를 실행해 주세요\n",
        "\n",
        "!wget -O /content/model_47.pt \"https://www.dropbox.com/s/g5nwt0do351huc2/model_47.pt?dl=1\"</br>\n",
        "!wget -O /content/model_48.pt \"https://www.dropbox.com/s/86qev1vgg545ee4/model_48.pt?dl=1\"</br>\n",
        "!wget -O /content/model_49.pt \"https://www.dropbox.com/s/obceyvw3d40rsxo/model_49.pt?dl=1\"</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWf3nYr2Umbt"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/model_49.pt \"https://www.dropbox.com/s/obceyvw3d40rsxo/model_49.pt?dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTPm3xhyW-tz"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load('./model_49.pt')\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbI7RChAWg9n"
      },
      "outputs": [],
      "source": [
        "out_train_image = model(bef_train_image.cuda())\n",
        "X_train = out_train_image\n",
        "Y_train = bef_train_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvt_yzLqVECP"
      },
      "outputs": [],
      "source": [
        "for test_image,test_label in test_data_loader :  \n",
        "  output_test = model(test_image.cuda())\n",
        "  X_test = output_test\n",
        "  Y_test = test_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ0acjmoXJIU"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
        "                     discriminant_analysis, random_projection)\n",
        "import torchvision.transforms.functional as Function\n",
        "from IPython.display import display\n",
        "from time import time\n",
        "from matplotlib import offsetbox\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_embedding(X, y_t, title=None):\n",
        "\n",
        "    y = y_t.numpy()\n",
        "    \n",
        "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "    X = (X - x_min) / (x_max - x_min)\n",
        "\n",
        "    plt.figure()\n",
        "    for i in range(X.shape[0]):\n",
        "      if i == 776 :\n",
        "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "               color='black',\n",
        "               fontdict={'weight': 'bold', 'size': 30})\n",
        "      else :\n",
        "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "                 color=plt.cm.Set1(y[i] / 10.),\n",
        "                 fontdict={'weight': 'bold', 'size': 9})\n",
        "\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    if title is not None:\n",
        "        plt.title(title)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiHZBzGoY0pW"
      },
      "outputs": [],
      "source": [
        "# 0~98\n",
        "check_index = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TYEuv2hHBRR"
      },
      "outputs": [],
      "source": [
        "test_image.shape\n",
        "test_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqBMAsiWXSF2"
      },
      "outputs": [],
      "source": [
        "image_query = X_test[check_index].view(1,-1)\n",
        "label_query = Y_test[check_index].view(1)\n",
        "X_total= torch.cat([X_train, image_query], dim=0)\n",
        "Y_total= torch.cat([Y_train, label_query], dim=0)\n",
        "\n",
        "test_img = Function.to_pil_image(test_image[check_index])\n",
        "print('테스트할 이미지 : {}'.format(test_label[check_index].item()))\n",
        "display(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qssqf7H1XUvI"
      },
      "outputs": [],
      "source": [
        "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
        "X_tsne = tsne.fit_transform(X_total.cpu().detach().numpy())\n",
        "\n",
        "pairwise_dist_t = _pairwise_distance(X_total)\n",
        "pairwise_dist_n = pairwise_dist_t.cpu().detach().numpy()\n",
        "\n",
        "pairwise_dist_sort = np.sort(pairwise_dist_n[-1][:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efxWLOCxXg3S"
      },
      "outputs": [],
      "source": [
        "test_img = Function.to_pil_image(test_image[check_index])\n",
        "print('들어온 이미지 : {}\\n'.format(test_label[check_index].item()))\n",
        "display(test_img)\n",
        "print(\"\\n\")\n",
        "\n",
        "for ii in range(5):\n",
        "  idx = np.where( pairwise_dist_n[-1] == pairwise_dist_sort[ii])\n",
        "  print('{} 번째로 비슷한 {} 번째 이미지 : '.format(ii+1,idx[0][0]))\n",
        "  img = Function.to_pil_image(bef_train_image[idx[0][0]])\n",
        "  #     img.save('test{}_{}.png'.format(Y_total[ii],ii))\n",
        "  display(img)\n",
        "  print(\"\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbmieku0C0n7"
      },
      "source": [
        "## 해당 그래프 그리는 방법은 TFKR 김대하님의 Github를 참고했습니다. \n",
        "\n",
        "# [바로가기](https://github.com/kdhht2334/Triplet_loss_for_image_retrieval?fbclid=IwAR2sBMRu6ae7XCMBfotevsttCDQGV5DcUvBLkUOS93izbpXlR7JrRzkJYvc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7iH93-QXooK"
      },
      "outputs": [],
      "source": [
        "plot_embedding(X_tsne, Y_total,\"Sejong Landmark data set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFOzaABybQvY"
      },
      "outputs": [],
      "source": [
        "X_tsne.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQcGyhMavtlx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
